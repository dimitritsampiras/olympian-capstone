\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{enumitem, amssymb}
\usepackage{changepage}
\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}
\hypersetup{
	colorlinks,
	citecolor=blue,
	filecolor=black,
	linkcolor=red,
	urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}
	
	\title{Project Title: System Verification and Validation Plan for \progname{}} 
	\author{\authname}
	\date{\today}
	
	\maketitle
	
	\pagenumbering{roman}
	
	\section{Revision History}
	
	\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
		\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
		\midrule
		02-11-2022 & 1.0 & Initial Draft\\
		\bottomrule
	\end{tabularx}
	
	\newpage
	
	\section{Reference Material}
	
	This section records information for easy reference.
	
	\subsection{Abbreviations and Acronyms}
	
	\renewcommand{\arraystretch}{1.2}
	\begin{tabular}{l l} 
		\toprule		
		\textbf{symbol} & \textbf{description}\\
		\midrule 
		AC & Anticipated Change\\
		DAG & Directed Acyclic Graph \\
		M & Module \\
		MG & Module Guide \\
		MIS & Module Interface Specification \\
		OS & Operating System \\
		R & Requirement\\
		RPE & \href{https://en.wikipedia.org/wiki/Rating_of_perceived_exertion}{Rating of Perceived Exertion} \\
		SC & Scientific Computing \\
		SRS & Software Requirements Specification\\
		\progname & The Capstone course this project belongs to \\
		UC & Unlikely Change \\
		\bottomrule
	\end{tabular}\\
	
	\newpage
	
	\tableofcontents
	
	\listoftables
	
	\newpage
	
	\newpage
	
	\pagenumbering{arabic}
	
	This document serves to document the plan place to verify and validate the development of the Olympian Application.
	
	\section{General Information}
	
	\subsection{Summary}
	
	%\wss{Say what software is being tested.  Give its name and a brief overview of
	%	its general functions.}
	
	The software being tested is a mobile application called Olympian. It is a workout application with a social component.
	Important components of the software that will be getting tested are:

	\begin{itemize}
		\item The functionality to log in and sign up new users. This will function to allow users to create a customized experience with data being saved over between uses.
		\item The functionality for users to create and upload their own workouts. Specifically, users should be able to set a workout category, outline steps for their workout plan, and include videos and images to support the exercise.
		\item The functionality for users to discover new and popular workouts that have been created by other users. Users should be able to sort by new or trending and filter by the muscle groups that they would like to target.
	\end{itemize}
	
	\subsection{Objectives}
	
	%\wss{State what is intended to be accomplished.  The objective will be around
	%	the qualities that are most important for your project.  You might have
	%	something like: ``build confidence in the software correctness,''
	%	``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
	%	just those that are most important.}

		\begin{itemize}
			\item Demonstrate ease of use for new users to sign in.
			\item Show off intuitiveness and learnability of the workout creation process.
			\item Demonstrate usability of workout feed. Newly created workouts must appear in this feed.
			\item Support the claim that our client shall update the server and vice versa in under 10 seconds under average North American internet speeds.
		\end{itemize}

	\subsection{Relevant Documentation}

	\begin{enumerate}
		\item \href{https://github.com/dimitritsampiras/olympian/blob/main/docs/SRS/SRS.pdf}{SRS} 
		\item \href{https://github.com/dimitritsampiras/olympian/blob/main/docs/Design/MG/MG.pdf}{MG} 
		\item \href{https://github.com/dimitritsampiras/olympian/blob/main/docs/Design/MIS/MIS.pdf}{MIS} 
	\end{enumerate}
	
	%\wss{Reference relevant documentation.  This will definitely include your SRS
	%	and your other project documents (design documents, like MG, MIS, etc).  You
	%	can include these even before they are written, since by the time the project
	%	is done, they will be written.}
	
	\section{Plan}

	This section contains the criteria for reviewing and verifying the various artifacts involved with the project.
	
	% \wss{Introduce this section.   You can provide a roadmap of the sections to
	% 	come.}
	
	\subsection{Verification and Validation Team}

	% \wss{Your teammates.  Maybe your supervisor.
	% 	You shoud do more than list names.  You should say what each person's role is
	% 	for the project's verification.  A table is a good way to summarize this information.}

	\begin{table}
		\begin{center}
			\begin{tabular}{ |c|c| } 
				\hline
				\textbf{Team Member} & \textbf{Role} \\
				\hline
				Jared Bentvelsen & Integration Testing for final mobile app end-to-end testing \\
				\hline 
				Yuvraj Randhawa & Integration Testing for final mobile app end-to-end testingg \\
				\hline
				Bassel Rezkalla & Functional Unit Testing for mobile app back-end and React front-end \\
				\hline
				Matthew McCracken & Non Functional Unit Testing for both back-end server to front-end \\
				\hline
				Dimitri Tsampiras & UI/UX Verification Testing \\
				\hline 
				William Lee & Code Quality and Optimization Verification\\
				\hline
			\end{tabular}
			\caption{Team Member Roles}
		\end{center}
	\end{table}
	
	\subsection{SRS Verification Plan}
	% \wss{List any approaches you intend to use for SRS verification.  This may include
	% 	ad hoc feedback from reviewers, like your classmates, or you may plan for 
	% 	something more rigorous/systematic.}
	% \wss{Maybe create an SRS checklist?}

	The SRS will be evaluated by team members and classmates as the project progresses. It will also be verified through testing with a traceability matrix.
	GitHub issues left by other groups will be carefully considered and addressed.
	The following checklist will be used:
	\begin{todolist}
		\item Can the system clearly demonstrate each requirement?
		\item Can the system demonstrate each requirement outside of a testing or debugging environment?
		\item Does each requirement map to one or more tests?
	\end{todolist}
	
	\subsection{Design Verification Plan}
	% \wss{Plans for design verification}
	% \wss{The review will include reviews by your classmates}
	% \wss{Create a checklists?}

	The project design will be manually reviewed by each team member and classmates. The following checklist will be used:
	\begin{todolist}
		\item Does the system design allow for each functional and non functional requirement to be met?
		\item Does the chosen architecture(s) make sense for the given use cases? Does it allow for easy scalability?
		\item Does the design follow appropriate design principles (e.g. low coupling, high cohesion, separation of concerns)?
	\end{todolist}
	
	\subsection{Verification and Validation Plan Verification Plan}
	% \wss{The verification and validation plan is an artifact that should also be verified.}
	% \wss{The review will include reviews by your classmates}
	% \wss{Create a checklists?}

	The Verification and Validation Plan artifact will be read and reviewed by each team member according to the following checklist:
	\begin{todolist}
		\item Are there any missing or empty sections?
		\item Is each section complete with sufficient detail?
		\item Are there any contradictions between sections?
		\item Is the table of contents present and reflective of the document's sections?
		\item Is the revision history up to date?
		\item Are there any typos in the document?
	\end{todolist}
	GitHub issues left by other groups will be carefully considered and addressed.
	
	\subsection{Implementation Verification Plan}
	
	% \wss{You should at least point to the tests listed in this document and the unit
	% 	testing plan.}
	
	% \wss{In this section you would also give any details of any plans for static verification of
	% 	the implementation.  Potential techniques include code walkthroughs, code
	% 	inspection, static analyzers, etc.}

	Implementation Verification will be done according to the functional and non-functional tests listed in this document.
	Code additions will be thoroughly reviewed by team members in feature branches before merging with the main branch.
	Linters and formatters will be utilized to ensure code consistency.
	
	\subsection{Automated Testing and Verification Tools}
	
	See Section 3.5 of Problem Statement.
	% \wss{What tools are you using for automated testing.  Likely a unit testing
	% 	framework and maybe a profiling tool, like ValGrind.  Other possible tools
	% 	include a static analyzer, make, continuous integration tools, test coverage
	% 	tools, etc.  Explain your plans for summarizing code coverage metrics.
	% 	Linters are another important class of tools.  For the programming language
	% 	you select, you should look at the available linters.  There may also be tools
	% 	that verify that coding standards have been respected, like flake9 for
	% 	Python.}
	% \wss{If you have already done this in the development plan, you can point to
	% 	that document.}
	% \wss{The details of this section will likely evolve as you get closer to the
	% 	implementation.}
	
	\subsection{Software Validation Plan}
	% \wss{If there is any external data that can be used for validation, you should
	% 	point to it here.  If there are no plans for validation, you should state that
	% 	here.}
	% \wss{You might want to use review sessions with the stakeholder to check that
	% 	the requirements document captures the right requirements.  Maybe task based
	% 	inspection?}
	% \wss{This section might reference back to the SRS verification section.}
	No external data will be used for validation. In addition to the SRS verification checklist, meetings will be held with stakeholders to ensure
	requirements cover all desired tests.
	\section{System Test Description}
	
	\subsection{Tests for Functional Requirements}
	
	% \wss{Subsets of the tests may be in related, so this section is divided into
	% 	different areas.  If there are no identifiable subsets for the tests, this
	% 	level of document structure can be removed.}
	
	% \wss{Include a blurb here to explain why the subsections below
	% 	cover the requirements.  References to the SRS would be good here.}

	The following tests correspond to functional requirements outlined in the SRS.
	
	% \wss{It would be nice to have a blurb here to explain why the subsections below
	% 	cover the requirements.  References to the SRS would be good here.  If a section
	% 	covers tests for input constraints, you should reference the data constraints
	% 	table in the SRS.}
	
	\begin{enumerate}
		
		\subsubsection{Workout Routine Tests}
		\item{\textbf{test-WR-1}}: A Workout Routine can be created
		
		Control: Manual
		
		Initial State: Application is running.
		
		Input: \ref{WR-1-input}
		
		Output: A workout routine is stored in the database and is accessible to the user that created it. If the user determined that the workout to be public then it should be publicly visible.
		
		Test Case Derivation: On a successful creation, the workout routine should be placed into the database.
		
		How test will be performed: This test can be preformed with a manual insertion of creating a workout routine under a test user. These results can be determined by viewing the database and scope of visibility of the created routine.
		
		Requirement(s): R1
		
		\item{\textbf{test-WR-2}}: Editing a Workout Routine
		
		Control: Manual
		
		Initial State: Application is running with an existing workout routine.
		
		Input: \ref{WR-1-input}
		
		Output: A workout routine is updated with new values in the database and new values are visible to accessible users.
		
		Test Case Derivation: On a successful edit, the workout routine should be updated with new values database.
		
		How test will be performed: This test can be preformed with a manual edit of a workout routine under a test user.
		
		Requirement(s): R1
		
		\subsubsection{Exercise Tests}
		\item{\textbf{test-EX-1}}: Adding an Exercise to a Workout Routine

		Control: Manual
		
		Initial State: Application is running and a workout routine is in the process of being created or edited.
		
		Input: A user created exercise with the parameters required for the exercise.
		
		Output: The workout routine should include the added exercise.
		
		Test Case Derivation: Because exercises are bounded to a specific workout routine, the routine that the exercise was created under should include the new exercise.
		
		How test will be performed: Given a workout routine, an exercise will be manually added to determine if it is linked and added to the workout routine.
		
		Requirement(s): R2
		
		
		\item{\textbf{test-EX-2}}: Removing an Exercise from a Workout Routine
		
		Control: Manual
		
		Initial State: Application is running and a workout routine is in the process of being created or edited.
		
		Input: The user chooses to remove an exercise.
		
		Output: Some feedback indicating that an exercise has been removed. The workout routine no longer contains the removed exercise.
		
		Test Case Derivation: On removal of an exercise, there should be an indication towards the user notifying them of the removal. The exercise should also be deleted from the workout routine.
		
		How test will be performed: Given a workout routine, an exercise will be manually removed and checked to ensure that it no longer exists within the given workout routine.
		
		Requirement(s): R2
		
		
		\item{\textbf{test-EX-3}}: Limiting Exercises to a Workout Routine
		
		Control: Manual
		
		Initial State: Application is running and a workout routine is in the process of being created or edited.
		
		Input: The number of exercises required to reach the limit of exercises per workout routine (the limit is set to 10 exercises). 
		
		Output: A notification to the user, before the limiting exercise notifying them of the limit, and preventing them from adding another exercise.
		
		Test Case Derivation: The user should be aware of the exercise limit per workout routine and should not be able to exceed it.
		
		How test will be performed: The test will be preformed by manually added exercises to a workout routine until the limit. Then upon adding the limiting input, a notification is expected.
		
		Requirement(s): R2
		
		\subsubsection{Quantifier Tests}
		\item{\textbf{test-QT-1}}: Adding Quantifiers to an Exercise
		
		Control: Manual
		
		Initial State: The application is running and an exercise is in the process of being created or edited.
		
		Input: \ref{QT-1-input}
		
		Output: The exercise now stores the inputted sets, along with their associated number of sets and RPE.
		
		Test Case Derivation: On adding a quantifier to an exercise, there should be a unit of measurement and value associated with it.
		
		How test will be performed: This test will be executed by adding a quantifier to various types of exercises to ensure they correctly measure the exercise.
		
		Requirement(s): R3
		
		\item{\textbf{test-QT-2}}: Removing Quantifiers from an Exercise
		
		Control: Manual
		
		Initial State: The application is running and an exercise is in the process of being created or edited.
		
		Input: Removal of a quantifier.
		
		Output: The exercise no longer holds a quantifier and is not displayed to the user.
		
		Test Case Derivation: On removing a quantifier to an exercise, it should no longer be visible to the user.
		
		How test will be performed: This test will be executed by removing a quantifier from an exercises to ensure it does not display any quantifying information.
		
		Requirement(s): R3
		\item{\textbf{test-QT-3}}: Editing Quantifiers of an Exercise
		
		Control: Manual
		
		Initial State: The application is running and an exercise is in the process of being created or edited.
		
		Input: \ref{QT-1-input}
		
		Output: The exercise now holds the updated quantifier.
		
		Test Case Derivation: On updating a quantifier to an exercise, there should be a new unit of measurement and/or value associated with it.
		
		How test will be performed: This test will be executed by editing a quantifier and ensuring the values are updated on completion of the edit.
		
		Requirement(s): R3
		
		\subsubsection{Publicity Tests}
		\item{\textbf{test-PB-1}}: Publicizing a Workout Routine
		
		Control: Manual
		
		Initial State: The application is running and a private workout routine is created.
		
		Input: The user sets the workout routine's publicity setting to "public".
		
		Output: The workout routine is declared public in the database and is now visible to all users.
		
		Test Case Derivation: This is the expected output because if the workout routine was not public before this action then it should not be visible to users. Once updating the publicity, the database should be updated and the routine should be publicly available 
		
		How test will be performed: This test will be done by manually changing the state of a workout routine and checking for a database update and viewing the routine under a different test-user.
		
		Requirement(s): R4
		
		\item{\textbf{test-PB-2}}: Privatizing a Workout Routine
		
		Control: Manual
		
		Initial State: The application is running and a public workout routine is created.
		
		Input: An edit or addition to a workout routine to make is private.
		
		Output: The workout routine is declared private in the database and is no longer visible to any user except the creator.
		
		Test Case Derivation: By privatizing a routine, it should no longer be accessible to any other user than the creator. There should also be a database update to signify this.
		
		How test will be performed: This test will be done by manually changing the state of a workout routine and checking for a database update and viewing the routine under a different test-user.
		
		Requirement(s): R4
		
		\subsubsection{Workout Routine Saving Tests}
	
		\item{\textbf{test-WS-1}}: Saving a Public Workout Routine
		
		Control: Manual
		
		Initial State: The application is running and there is a public workout routine created by another user.
		
		Input: The other user workout routine is saved.
		
		Output: The workout routine is now visible and accessible under the saved workout routines.
		
		Test Case Derivation: On saving a workout routine, it should be stored somewhere the user can access it for later use.
		
		How test will be performed: This test will be done by creating a public workout routine on one test-user. On a different test user this routine will be saved. The saved workout routines should then include the newly saved one.
		
		Requirement(s): R5
		
		\subsubsection{Browsing Workout Routine Tests}
		\item{\textbf{test-BS-1}}: Browsing Workout Routines
		
		Control: Manual
		
		Initial State: The application is running and multiple workout routines are created.
		
		Input: navigation movements to view the public workout routines.
		
		Output: Multiple public workout routines should be displayed. 
		
		Test Case Derivation: On making a routine public, it should be available to all users and also the browsing page.
		
		How test will be performed: By first adding multiple public test routines, then checking the public workout routines by browsing. There should exist routines to access and view.
		
		Requirement(s): R6
		
		\item{\textbf{test-BS-2}}: Search Workout Routines
		
		Control: Manual
		
		Initial State: The application is running and multiple workout routines are created.
		
		Input: Search string inputs to view the public workout routines.
		
		Output: Public workout routines that match the search criteria should be displayed. 
		
		Test Case Derivation: All workout routines that match a search, should be displayed.
		
		How test will be performed: By first adding multiple public test routines, then preforming a search, the routines that match the search criteria should be displayed.
		
		Requirement(s): R6
		
		\subsubsection{User Profile Tests}
		\item{\textbf{test-UP-1}}: Creating a User Profile
		
		Control: Manual
		
		Initial State: The application is running.
		
		Input: The required parameters for creating a profile.
		
		Output: The created profile is stored in a user database and the user should be logged into their profile.
		
		Test Case Derivation: On creating a profile the database must store the information. To go along with this the user must maintain the login status of the created profile.
		
		How test will be performed: On launching the application without a created profile a profile will be created. The database will then be checked to ensure the proper data matches.
		
		Requirement(s): R7
		
		\item{\textbf{test-UP-2}}: Viewing Other User Profile
		
		Control: Manual
		
		Initial State: The application is running and there exists another user profile.
		
		Input: Search criteria for the searched user profile.
		
		Output: A user profile is displayed with their public routines, fitness goals, and other public profile data.
		
		Test Case Derivation: On searching for another user the data shown should only be what is made public.
		
		How test will be performed: By creating a test-user profile then searching for it. The profile should display all public information and hide non-public information. 
		
		Requirement(s): R8
		
		\subsubsection{Fitness Goal Tests}
		
		\item{\textbf{test-FG-1}}: Creating a Fitness Goal
		
		Control: Manual
		
		Initial State: The application is running and a profile has been created or is in the process of creation.
		
		Input: A string parameter for a fitness goal and publicity options for the goal.
		
		Output: The fitness goal for the user is now updated in the database and is saved under their profile.
		
		Test Case Derivation: The fitness goals are specific to a given user so the data for fitness goals are stored under their profile data.
		
		How test will be performed: A test user will add a fitness goal and check if it is displayed on their profile.
		
		Requirement(s): R9
		
		\item{\textbf{test-FG-2}}: Progressing a Fitness Goal
		
		Control: Manual
		
		Initial State: The application is running and a fitness goal is created.
		
		Input: Progress points towards a given fitness goal at a given date.
		
		Output: The progress displayed towards a fitness goal should be visually updated and numerically updated in the database with a date.
		
		Test Case Derivation: The data for a fitness goal must be updated in the database for storage. To follow this, to goal must also be visually updated accordingly to signify progress towards the goal at the given date.
		
		How test will be performed: A test-user will create a fitness goal and add progress points towards the goal. There should be a database update and visual update to signify this change.
		
		Requirement(s): R10, R11
		
		
	\end{enumerate}

	
	\subsection{Tests for Nonfunctional Requirements}
	
	% \wss{The nonfunctional requirements for accuracy will likely just reference the
	% 	appropriate functional tests from above.  The test cases should mention
	% 	reporting the relative error for these tests.  Not all projects will
	% 	necessarily have nonfunctional requirements related to accuracy}
	
	% \wss{Tests related to usability could include conducting a usability test and
	% 	survey.  The survey will be in the Appendix.}
	
	% \wss{Static tests, review, inspections, and walkthroughs, will not follow the
	% 	format for the tests given below.}
	
	\subsubsection{Look and Feel Testing}
	
	
	\begin{enumerate}
		
		% \item{\textbf{test-id-1}}: TEMPLATE
		
		% Type: Functional, Dynamic, Manual, Static etc.
		
		% Initial State: 
		
		% Input/Condition: 
		
		% Output/Result: 
		
		% How test will be performed: 
		
		% Requirement(s):
			
		\item{\textbf{test-LF-1}}: Style

		Type: Manual.
		
		Initial State: The application is running.
		
		Input/Condition: A survey is conducted with various user interactions with the application. 
		
		Output/Result: The survey results will record the overall style quality targeting areas such as simplicity, cleanliness and aesthetic appeal. 
		
		How test will be performed: This test will be preformed by conducting a survey to determine if test users think that the products appearance is minimal and straight forward.
		
		Requirement(s): NFR1
		
		

	
	\subsubsection{Usability and Humanity Tests}
		\item{\textbf{test-UH-1}}: Text Sizing and Visibility.
		
		Type: Manual
		
		Initial State: The application is running.
		
		Input/Condition: Various application screens are shown to users of diverse demographics.
		
		Output/Result: The survey results will give an indication for how visible application text is, and if any specific text needs adjustment.
		
		How test will be performed: A survey is conducted where users of various demographics interact with the application and are asked how visible application text is.
		
		Requirement(s): NFR2
		
		\item{\textbf{test-UH-2}}: Text Language.
		
		Type: Manual
		
		Initial State: The application is running.
		
		Input/Condition: User click input of language from dropdown menu.

		Output/Result: Language of the application changes to their users' chosen language.
		
		How test will be performed: Language dropdown will ba manually pressed and a language will be chosen. The application should change all text to the new selected language.
		
		Requirement(s): NFR3
		
		\item{\textbf{test-UH-3}}: Learnability.
		
		Type: Manual
		
		Initial State: The application is running.
		
		Input/Condition: The system is given to a group of users to use for a predetermined time period.
		
		Output/Result: The survey will give an indication of how easy or hard the system is to learn.
		
		How test will be performed: Each user will be tasked with performing a specific task within a set amount of time, and report how easy or difficult the task was to learn to perform.
		
		Requirement(s): NFR4
		
		\item{\textbf{test-UH-4}}: Understandability.
		
		Type: Manual

		Initial State: The application is running.
		
		Input/Condition: Various system outputs are shown to a group of users in a survey.
		
		Output/Result: The survey will give an indication of how easy or hard system output is to understand.
		
		How test will be performed: Each user in the survey is shown various system outputs and asked how easy the outputs are to understand.
		
		Requirement(s): NFR5
		
		\item{\textbf{test-UH-5}}: Hearing and Audio considerations.
		
		Type: Manual
		
		Initial State: The application is running.
		
		Input/Condition: Each system sound is played to a group of users in a survey.
		
		Output/Result: The survey will give an indication on how audible/pleasant the system sounds are.
		
		How test will be performed: System sounds will be played to users in a survey, who will respond will their perception of the audio's quality.
		
		Requirement(s): NFR6
		
		\item{\textbf{test-UH-6}}: Use of Colour and Contrast.
		
		Type: Manual
		
		Initial State: Application is running.
		
		Input/Condition: Users are shown images of various application views.
		
		Output/Result: The survey will give an indication of how visible application views are with respect to colour and contrast.
		
		How test will be performed: Users will be shown images of various application views in a survey and asked to judge the visibility based on colour and contrast.
		
		Requirement(s): NFR7
	\subsubsection{Performance Tests}
		\item{\textbf{test-PF-1}}: Speed and Latency.
		
		Type: Dynamic
		
		Initial State: Application will be running.
		
		Input/Condition: String representing workout name and workout information.
		
		Output/Result: Inputted information updated existing program or new program created and displayed.
		
		How test will be performed: Program will be inputted and updated utilizing a testing framework that will time whether starting to completion took 10 seconds or less. 
		
		Requirement(s): NFR8
		
		\item{\textbf{test-PF-2}}: Accuracy and Precision of Quantifiers.
		
		Type: Manual
		
		Initial State: The application will be running and on the post search page.
		
		Input/Condition: Search string inputs to view public posts.
		
		Output/Result: Search results with ratings at an accuracy of 2 decimal places.
		
		How test will be performed: A string will be used to search public posts and all posts will be checked for two decimal places.
		
		Requirement(s): NFR9
		
		\item{\textbf{test-PF-3}}: Availability and Uptime
		
		Type: Manual
		
		Initial State: A server will be running and hosting our app to mobile clients.
		
		Input/Condition: Apache JMeter will perform a variety of tasks across multiple clients.
		
		Output/Result: The application will remain live for \%95+ of the duration of the testing.
		
		How test will be performed: Apache JMeter will perform a variety of tasks across multiple users to simulate strain for an extended duration. Uptime.com will be used to check the uptime of the application over the test period.
		
		Requirement(s): NFR10
		
		\item{\textbf{test-PF-4}}: User Capacity.
		
		Type: Automatic
		
		Initial State: A server will be running and hosting our app to mobile clients.
		
		Input/Condition: Apache JMeter will begin running 100 clients and send each client an update.
		
		Output/Result: The server will receive all responses within 5 seconds.
		
		How test will be performed: JMeter will perform capacity testing to test if the server can handle up to 100 clients. It will attempt to send an update to each client and receive a response.
		
		Requirement(s): NFR11
		
		\item{\textbf{test-PF-5}}: Scalability of User Capacity.
		
		Type: Automatic
		
		Initial State: A server will be running and hosting our app to mobile clients a year after our release.
		
		Input/Condition: Apache JMeter will begin running 100 clients.
		
		Output/Result: All client updates will be successfully performed.
		
		How test will be performed: JMeter will have all 1000 clients perform simultaneous user updates and tasks and expect task completion within 10 seconds.
		
		Requirement(s): NFR12
	\subsubsection{Operational and Environment Tests}
		\item{\textbf{test-OE-1}}: Supported Systems.
		
		Type: Manual
		
		Initial State: The application is closed and the mobile device is turned on.
		
		Input/Condition: The device downloads and launches the application. All required hardware features are tested on iOS and Android devices.
		
		Output/Result: There should be complete compatibility for both supported systems without any operation system errors. 
		
		How test will be performed: This test will be done by downloading and running all features on both an iOS and Android device ensuring there are no operating system errors.
		
		Requirement(s): NFR13
	\subsubsection{Maintainability and Support Tests}
		\item{\textbf{test-MS-1}}: Maintenance.
		
		Type: Manual
		
		Initial State: The application is running. 
		
		Input/Condition: The server will be set to activate maintenance mode at a set date (within 24 hours).
		
		Output/Result: Users currently in the application will see a maintenance alert.
		
		How test will be performed: A simulated maintenance event will be triggered and application users will verify that a maintenance banner is shown to clients.
		
		Requirement(s): NFR14
	\subsubsection{Security Tests}
		\item{\textbf{test-SEC-1}}: Private and Public Details.
		
		Type: Manual
		
		Initial State: The application will be running and logged into a user profile. 
		
		Input/Condition: Navigate to public user profiles.
		
		Output/Result: Other users profiles should not display any private details such as their passwords or email information. 
		
		How test will be performed: Search for other user profiles and verify that no private information is being displayed on their profile to other users.
		
		Requirement(s): NFR15
		
		\item{\textbf{test-SEC-2}}: Passwords.
		
		Type: Manual 
		
		Initial State: User is logged out (i.e. not authenticated and is forbidden from accessing account).
		
		Input: String parameter as password alongside corresponding account.
		
		Output: Assuming username exists - inputted password is tested against hashed version of actual user password. If verification is successful, user is logged in. 
		
		Test Case Derivation: The user is redirected to indicate a successful login. User is notified in the case of an unsuccessful login (e.g. wrong password).
		
		How test will be performed: Test whether or not username exists. Test inputted password against account password. Test if correct error message is presented.
		
		Requirement(s): NFR16
		
		\item{\textbf{test-SEC-3}}: Client Server Privacy.
		
		Type: Manual 
		
		Initial State: User request is made.
		
		Input: User request has authorization token in request header .
		
		Output: Server process authorization token. If authorized, server request will continue. If not, server returns 403 server error (forbidden).
		
		Test Case Derivation: Test authencation token against server token secret. 
		
		How test will be performed: Extract authorization token from header. If header exists, run token verification function.
	
		Requirement(s): NFR17
		
		\item{\textbf{test-SEC-4}}: Data storage and logging.
		
		Type: Functional
		
		Initial State: Application is running 
		
		Input/Condition: A valid data entry for a workout routine or user is entered into the database.
		
		Output/Result: The entry is successfully added and there is a database recorded log of the interaction.
		
		How test will be performed: As a data point gets added, there should exist an output log for each interaction, this will be checked by viewing database logs. This will only look at valid data entries because these should get filtered out via unit testing.
		
		Requirement(s): NFR18
		
		\item{\textbf{test-SEC-5}}: Data Backups.
		
		Type: Functional, Manual
		
		Initial State: Application is running
		
		Input/Condition: Data entries exist in the database and the database fails.
		
		Output/Result: There should exist a backup of the database from a recent saved state.
		
		How test will be performed: This can be preformed manually by shutting down the database and checking if the backup is used. This can also be preformed by checking if the backup exists and is refreshed occasionally.
		
		Requirement(s): NFR18
	\subsubsection{Cultural Requirements Tests}
		\item{\textbf{test-CR-1}}: Profanity and Inappropriate Language.
		
		Type: Functional
		
		Initial State: The application is running.
		
		Input/Condition: A user posts content which contains profanities.
		
		Output/Result: The profanity is filtered out by the system, and the user is suspended from the application if they are a repeat offender.
		
		How test will be performed: The action of posting a piece of content will be manually simulated, and content will be posted containing profanities. The content will be checked to verify filtering of profane words, and the user's account status will be checked to verify suspension on a repeat offense.
		
		Requirement(s): NFR19
		
		\item{\textbf{test-CR-2}}: Reporting Offensive Language.
		
		Type: Manual
		
		Initial State: The application is running.
		
		Input/Condition: A user is viewing the context options for interacting with a piece of recommended content on their feed (which they may find offensive).
		
		Output/Result: The user is able to report the piece of offensive content, removing it from their feed. The content should no longer be visible on the user's feed.
		
		How test will be performed: The action of interacting with context options for a piece of recommended content will be manually simulated, and an attempt will be made to report the content, making it no longer visible on the user's feed.
		
		Requirement(s): NFR20
		
	\subsubsection{Legal Requirements Tests}
		\item{\textbf{test-LR-1}}: Age and Gender Use.
		
		Type: Manual
		
		Initial State: The application is running.
		
		Input/Condition: The user is personalizing their profile.
		
		Output/Result: The user has the ability to specify their gender, age, and race.
		
		How test will be performed: The profile personalization process will be manually simulated, and an attempt will be made to specify a sample gender, age, and race.
		
		Requirement(s):NFR21
		\item{\textbf{test-LR-2}}: Data Protection.
		
		Type: Static
		
		Initial State: The application database contains sensitive user data.
		
		Input/Condition: N/A
		
		Output/Result: All usage and storage of user data is compliant with the Data Protection Act.
		
		How test will be performed: The logic used to store data will be analyzed to ensure that no user data is stored unlawfully or unfairly, data will be kept only as long as the user consents, and that no sensitive data is transferred across networks in an unencrypted state.
		
		Requirement(s): NFR22
	\end{enumerate}
	\subsection{Traceability Between Test Cases and Requirements}
	
	% \wss{Provide a table that shows which test cases are supporting which
	% 	requirements.}
	\newpage
		\section{Traceability Matrices and Graphs}
	
	\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
			\hline
			& R1 & R2 & R3 & R4 & R5 & R6 & R7 & R8 & R9 & R10 & R11 \\ \hline
			WR1 &X & & & & & & & & & & \\ \hline
			WR2 &X & & & & & & & & & & \\ \hline
			EX1 & &X & & & & & & & & & \\ \hline 
			EX2 & &X & & & & & & & & & \\ \hline
			EX3 & &X & & & & & & & & &\\ \hline 
			QT1 & & &X & & & & & & & & \\ \hline 
			QT2 & & &X & & & & & & & & \\ \hline 
			QT3 & & &X & & & & & & & & \\ \hline 
			PB1 & & & &X & & & & & & & \\ \hline 
			PB2 & & & &X & & & & & & & \\ \hline 
			WS1 & & & & &X & & & & & & \\ \hline 
			BS1 & & & & & &X & & & & & \\ \hline 
			BS2 & & & & & &X & & & & & \\ \hline 
			UP1 & & & & & & &X & & & & \\ \hline 
			UP2 & & & & & & & &X & & & \\ \hline 
			FG1 & & & & & & & & &X & & \\ \hline 
			FG2 & & & & & & & & & &X &X \\ \hline 	
		\end{tabular}
		\caption{Functional System Tests to Functional Requirement Matrix}
		\label{Table:R_trace}
	\end{table}

\begin{table}[h!]
	\begin{adjustwidth}{-3cm}{-5cm}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
			\hline
			& NFR1 & NFR2 & NFR3 & NFR4 & NFR5 & NFR6 & NFR7 & NFR8 & NFR9 & NFR10 & NFR11 \\ \hline
			LF1 &X & & & & & & & & & & \\ \hline
			UH1 & &X & & & & & & & & & \\ \hline
			UH2 & & &X & & & & & & & & \\ \hline
			UH3 & & & &X & & & & & & & \\ \hline 
			UH4 & & & & &X & & & & & & \\ \hline 
			UH5 & & & & & &X & & & & & \\ \hline 
			UH6 & & & & & & &X & & & & \\ \hline 
			PF1 & & & & & & & &X & & & \\ \hline 
			PF2 & & & & & & & & &X & & \\ \hline 
			PF3 & & & & & & & & & &X & \\ \hline
			PF4 & & & & & & & & & & &X \\ \hline  
		\end{tabular}
		\caption{Non-Functional System Tests to Non-Functional Requirement Matrix}
		\label{Table:R_trace}
		
				\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
			\hline
			& NFR12 & NFR13 & NFR14 & NFR15 & NFR16 & NFR17 & NFR18 & NFR19 & NFR20 & NFR21 & NFR22 \\ \hline
			PF5 &X & & & & & & & & & & \\ \hline
			OE1 & &X & & & & & & & & & \\ \hline
			MS1 & & &X & & & & & & & & \\ \hline
			SEC1 & & & &X & & & & & & & \\ \hline 
			SEC2 & & & & &X & & & & & & \\ \hline 
			SEC3 & & & & & &X & & & & & \\ \hline 
			SEC4 & & & & & &X & & & & & \\ \hline 
			SEC5 & & & & & & &X & & & & \\ \hline 
			CR1 & & & & & & & &X & & & \\ \hline 
			CR2 & & & & & & & & &X & & \\ \hline 
			LR1 & & & & & & & & & &X & \\ \hline 
			LR2 & & & & & & & & & & &X \\ \hline 
		\end{tabular}
		\caption{Non-Functional System Tests to Non-Functional Requirement Matrix}
		\label{Table:R_trace}
	\end{adjustwidth}
	\end{table}
	\section{Unit Test Description}
	
	\wss{Reference your MIS (detailed design document) and explain your overall
		philosophy for test case selection.}  
	\wss{This section should not be filled in until after the MIS (detailed design
		document) has been completed.}
	
	\subsection{Unit Testing Scope}
	
	\wss{What modules are outside of the scope.  If there are modules that are
		developed by someone else, then you would say here if you aren't planning on
		verifying them.  There may also be modules that are part of your software, but
		have a lower priority for verification than others.  If this is the case,
		explain your rationale for the ranking of module importance.}
	
	\subsection{Tests for Functional Requirements}
	
	\wss{Most of the verification will be through automated unit testing.  If
		appropriate specific modules can be verified by a non-testing based
		technique.  That can also be documented in this section.}
	
	\subsubsection{Module 1}
	
	\wss{Include a blurb here to explain why the subsections below cover the module.
		References to the MIS would be good.  You will want tests from a black box
		perspective and from a white box perspective.  Explain to the reader how the
		tests were selected.}
	
	\begin{enumerate}
		
		\item{test-id1\\}
		
		Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
			be automatic}
		
		Initial State: 
		
		Input: 
		
		Output: \wss{The expected result for the given inputs}
		
		Test Case Derivation: \wss{Justify the expected value given in the Output field}
		
		How test will be performed: 
		
		\item{test-id2\\}
		
		Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
			be automatic}
		
		Initial State: 
		
		Input: 
		
		Output: \wss{The expected result for the given inputs}
		
		Test Case Derivation: \wss{Justify the expected value given in the Output field}
		
		How test will be performed: 
		
		\item{...\\}
		
	\end{enumerate}
	
	\subsubsection{Module 2}
	
	...
	
	\subsection{Tests for Nonfunctional Requirements}
	
	\wss{If there is a module that needs to be independently assessed for
		performance, those test cases can go here.  In some projects, planning for
		nonfunctional tests of units will not be that relevant.}
	
	\wss{These tests may involve collecting performance data from previously
		mentioned functional tests.}
	
	\subsubsection{Module ?}
	
	\begin{enumerate}
		
		\item{test-id1\\}
		
		Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
			be automatic}
		
		Initial State: 
		
		Input/Condition: 
		
		Output/Result: 
		
		How test will be performed: 
		
		\item{test-id2\\}
		
		Type: Functional, Dynamic, Manual, Static etc.
		
		Initial State: 
		
		Input: 
		
		Output: 
		
		How test will be performed: 
		
	\end{enumerate}
	
	\subsubsection{Module ?}
	
	...
	
	\subsection{Traceability Between Test Cases and Modules}
	
	\wss{Provide evidence that all of the modules have been considered.}
	
	\bibliographystyle{plainnat}
	
	\bibliography{../../refs/References}
	
	\newpage
	
	\section{Appendix}
	
	This is where you can place additional information.
	
	\subsection{Symbolic Parameters}
	
	The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
	Their values are defined in this section for easy maintenance.
	
	\subsection{Test Inputs / Outputs}
	\subsubsection{Workout Routine Input\label{WR-1-input}}
	\begin{itemize}
		\item Workout name
		\item Workout description
		\item List of static exercises
	\end{itemize}

	\subsubsection{Exercise Quantifier Input\label{QT-1-input}}
	\begin{itemize}
		\item List of sets
		\item Number of reps for each set
		\item RPE for each set
	\end{itemize}

	\subsection{Usability Survey Questions}
	\wss{add surveys here}
	
	\newpage{}
	\section*{Appendix --- Reflection}
	
	The information in this section will be used to evaluate the team members on the
	graduate attribute of Lifelong Learning.  Please answer the following questions:
	
	\begin{enumerate}
		\item What knowledge and skills will the team collectively need to acquire to
		successfully complete the verification and validation of your project?
		Examples of possible knowledge and skills include dynamic testing knowledge,
		static testing knowledge, specific tool usage etc.  You should look to
		identify at least one item for each team member.
		\subsection{Required Skills}
		\begin{itemize}
			\item Skill: \textbf{Static Testing}
			\\ Rationale: Static testing will be a very important part of successfully completing the verification and validation plan for many reasons. Static testing involves matching the requirements to the code. It also involves looking into code error and structure. Having a good structure to the code will help with matching requirements stated in the SRS and validation plan resulting in a more sound product.
			\\ Team member: William Lee
			
			\item Skill: \textbf{Functional Testing}
			\\ Rationale: Functional testing is a crucial component in a verification and validation plan. Functional testing is used as a form of black-box testing to verify that the system's components provide the expected output, as per the requirements set forth in the software requirements specification.
			\\ Team member: Bassel Rezkalla

			\item Skill: \textbf{Integration Testing}
			\\ Rationale: Integration testing is a quintessential process in any software application. Integration testing tests the interaction between components in a system as they are actually deployed. These tests will be especially important for testing user requests and database transactions.
			\\ Team member: Jared Bentvelsen
      
			\item Skill: \textbf{Dynamic Testing}
			\\ Rationale: Dynamic testing is vital to the verification and validation plan. Dynamic testing is executing test cases to determine and analyze how the system and variables that are not constant and change with time react.
			\\ Team member: Yuvraj Randhawa

			\item Skill: \textbf{Automated Load Testing}
			\\ Rationale: Setting up automated tests to load/capacity test the mobile application is crucial in achieving verification and validation because it is needed for meeting capacity requirements.
			\\ Team member: Matthew McCracken
			
			\item Skill: \textbf{UI/UX Testing}
			\\ Rationale: UI/UX testing is a very overlooked aspect of testing. It ensures that the UI renders as intended, at the correct times, and with the correct data. Testing UI can help verify that all component states (pending, error, complete, etc.) are accounted for and function correctly.
			\\ Team member: Dimitri Tsampiras
		\end{itemize}
		\item For each of the knowledge areas and skills identified in the previous
		question, what are at least two approaches to acquiring the knowledge or
		mastering the skill?  Of the identified approaches, which will each team
		member pursue, and why did they make this choice?
		\subsection{Acquiring Knowledge and Mastering skills}
		\begin{itemize}
			\item Skill: \textbf{Static Testing Knowledge}
			\\ Approach 1: Research and contrast various static testing methods such as walkthroughs, code reviews and inspections.
			\\ Approach 2: Refer to previous notes on Software Testing 3S03 at McMaster University regarding static testing methodology.
			\\ Verdict: William will use approach 1 to research what methods are the best catered to this project because often tutorials for testing methods go in depth on strategy for approaching static testing. With approach 2, there is only a surface knowledge level of static testing learned from the course. However approach 2 can be used as a good starting point.
			
			\item Skill: \textbf{Functional Testing Knowledge}
			\\Approach 1: Review and practice black-box testing and functional testing techniques from SWFRENG 3S03: Software Testing.
			\\Approach 2: Examine the functional testing techniques used by other social applications that share similarities in feature functionality and requirements with our application.
			\\Verdict: Bassel will use Approach 2 to learn about writing and implementing functional tests as it may provide information that is more relevant and specifically applicable to an application such as ours.
			
			\item Skill: \textbf{Dynamic Testing Knowledge}
			\\Approach 1: Consult notes from previous courses that went over software testing (SFWRENG 3S03: Software Testing) and revisit the relevant course assignments.
			\\ Approach 2: Utilize online resources to learn and improve dynamic testing skills and abilities.
			\\ Verdict: Yuvraj will use Approach 1 to improve his Dynamic Testing knowledge. He has chosen this strategy because the course provided relevant and detailed notes from a reputable source with a lot of background information to aid in the learning.
			
			\item Skill: \textbf{Integration Testing Knowledge}
			\\Approach 1: Review and practice integration testing from SWFRENG 3S03: Software Testing.
			\\Approach 2: Read online articles on integration testing and consult integration tests in well known open source products.
			\\Verdict: Jared will use Approach 2 to learn about integration testing since SFWRENG 3S03 did not cover it in sufficient detail to expand his skills.

			\item Skill: \textbf{Automated Load Testing Knowledge}
			\\Approach 1: Consult former co-workers who worked on mobile application load testing to gather best practices and tips.
			\\Approach 2: Engage with online articles and forums that discuss using the specific load testing tools that we will use.
			\\Verdict: Matthew will use Approach 2 to learn about load testing because this will give a broad range of knowledge to complete many different types of mobile tests.
			
			\item Skill: \textbf{UI/UX Testing Knowledge}
			\\Approach 1: Utilize user testing to determine if actual user experience is as intended. Tools such as surveys and questionaires can be used.
			\\Approach 2: Use UI testing libraries learn how to verify and validate the system's UI components.
			\\Verdict: Dimitri will use Approach 2 to ensure proper data and component states are being displayed correctly, and at correct times. 
			
		\end{itemize}

	\end{enumerate}
		
	
\end{document}